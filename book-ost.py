import streamlit as st
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns
from itertools import islice

import requests
import json

from sklearn.metrics.pairwise import cosine_similarity

from selenium import webdriver
from selenium.webdriver.common.by import By
import time

from googletrans import Translator

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')

from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

st.title('Book-OSTğŸ§')
st.divider()

data = pd.read_excel('final_data.xlsx',index_col=0)
st.header('í”„ë¡œì íŠ¸ ê¸°íš ë°°ê²½')
st.markdown('ê·¼ë˜ì— ë“¤ì–´Â **í•œêµ­ì¸ì˜ ë…ì„œëŸ‰ ê°ì†Œ**ì™€ **ì Šì€ ì¸µì˜ ë¬¸í•´ë ¥ ì €í•˜**ê°€ ì‚¬íšŒì  ë¬¸ì œë¡œ ë– ì˜¤ë¥´ê³  ìˆìŠµë‹ˆë‹¤. ì±…ì´ë‚˜ ì‹ ë¬¸ê³¼ ê°™ì€ ì¶œíŒë¬¼ë¡œ ì •ë³´ë¥¼ ìŠµë“í–ˆë˜ ê³¼ê±°ì™€ ë‹¬ë¦¬, ì˜¤ëŠ˜ë‚  ì‚¬ëŒë“¤ì€ ì±… ì´ì™¸ì˜ ìˆ˜ë§ì€ ì •ë³´ ë§¤ì²´ì™€ ë¯¸ë””ì–´ë¡œë¶€í„° ì •ë³´ë¥¼ ìŠµë“í•  ìˆ˜ ìˆê²Œ ë˜ë©° ìì—°ìŠ¤ëŸ½ê²Œ ë…ì„œëŸ‰ì´ ê°ì†Œí•´ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.')
st.markdown('ë¯¸ë””ì–´ë¥¼ í†µí•œ ì •ë³´ ìŠµë“ê³¼ ë‹¬ë¦¬, ë…ì„œëŠ” ì •ì œë˜ì§€ ì•Šì€ ì •ë³´ë¥¼ ìŠ¤ìŠ¤ë¡œ ì´í•´í•˜ê³  ìì‹ ì˜ ê²ƒìœ¼ë¡œ ìŠµë“í•˜ëŠ” ì§€ì  ê³¼ì •ì„ ê±°ì¹˜ê¸° ë•Œë¬¸ì— ë…ì„œê°€ ë¬¸í•´ë ¥ê³¼ ê°™ì€ ì§€ì  ëŠ¥ë ¥ ë°œë‹¬ì— ë§¤ìš° ì¤‘ìš”í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì Šì€ ì¸µì˜ ë¬¸í•´ë ¥ ì €í•˜ ë¬¸ì œì˜ ì›ì¸ì´ â€˜ë…ì„œëŸ‰ ê°ì†Œâ€™ì— ìˆë‹¤ëŠ” ì˜ê²¬ì´ ì œê¸°ë˜ê³  ìˆìŠµë‹ˆë‹¤.')
st.markdown('ì´ëŸ¬í•œ **í•œêµ­ì¸ì˜ ë…ì„œëŸ‰ ê°ì†Œ**ì™€ **ì Šì€ ì¸µì˜ ë¬¸í•´ë ¥ ì €í•˜**ì— ëŒ€í•˜ì—¬, ì €í¬ íŒ€ì€ **ë…ì„œì— ëŒ€í•œ í¥ë¯¸ë¥¼ ë†’ì´ê³  ë…ì„œë¥¼ ì¥ë ¤í•  ìˆ˜ ìˆëŠ” ë°©ì•ˆì„ ì œì‹œí•˜ëŠ” ê²ƒ**ì´ ë‘ ë¬¸ì œì˜ í•´ê²° ë°©ì•ˆì´ ë  ê²ƒì´ë¼ ìƒê°í–ˆìŠµë‹ˆë‹¤.')
st.markdown('')
st.image('ë©œë¡œë””ì±…ë°©.png')
st.markdown('')
st.markdown('''ì˜í™”ë‚˜ ë“œë¼ë§ˆì²˜ëŸ¼ **ì±…ì—ë„ ostê°€ í•„ìš”í•˜ë‹¤ëŠ” Jtbc ë©œë¡œë””ì±…ë°© í”„ë¡œê·¸ë¨**ìœ¼ë¡œë¶€í„° ì˜ê°ì„ ì–»ì–´, **ë„ì„œ ë§ì¶¤ ìŒì•… ì¶”ì²œ ì‹œìŠ¤í…œ**ì´ë¼ëŠ” ì£¼ì œë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤. ìì‹ ì´ ì½ê³  ìˆëŠ” ì±…ì„ ì…ë ¥í•˜ë©´ ì±…ê³¼ ì˜ ì–´ìš¸ë¦¬ëŠ” ìŒì•…ì„ ì¶”ì²œí•´ì¤Œìœ¼ë¡œì¨ **ì±…ì˜ ê°ì •ê³¼ ë‚´ìš©ì„ ìŒì•… í•¨ê»˜ ë”ìš± ê¹Šì´ ìŒë¯¸í•˜ëŠ” ë…ì„œ ê²½í—˜ì„ ì œê³µ**í•˜ê³ ì í•©ë‹ˆë‹¤. ì Šì€ ì¸µì—ê²Œ ì¹œìˆ™í•œ ìŒì•…ì„ ë…ì„œì™€ ê²°í•©í•¨ìœ¼ë¡œì¨ ë…ì„œì— ëŒ€í•œ í¥ë¯¸ì™€ ì¦ê±°ì›€ì„ ë”í•˜ê³ , ì¥ê¸°ì ìœ¼ë¡œ ë…ì„œë¥¼ ì¥ë ¤í•˜ëŠ” í•˜ë‚˜ì˜ ë¬¸í™”ì  ì„œë¹„ìŠ¤ê°€ ë  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ê³  ìˆìŠµë‹ˆë‹¤.''')

st.divider()

st.subheader('ğŸ“– ì±… ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ“–')
book_title = st.text_input('ì˜ˆì‹œ) ë‚ ì”¨ê°€ ì¢‹ìœ¼ë©´ ì°¾ì•„ê°€ê² ì–´ìš”')
if not book_title:
    st.stop()
st.success('âœ… Thank You')

rest_api_key = "41d651c93152d5ec054dc828cacfa671"
url = "https://dapi.kakao.com/v3/search/book"
header = {"authorization": "KakaoAK "+rest_api_key}
querynum = {"query": book_title}

response = requests.get(url, headers=header, params = querynum)
content = response.text
ì±…ì •ë³´ = json.loads(content)['documents'][0]

book = pd.DataFrame({'title': ì±…ì •ë³´['title'],
              'isbn': ì±…ì •ë³´['isbn'],
              'authors': ì±…ì •ë³´['authors'],
              'publisher': ì±…ì •ë³´['publisher']})

target_url = ì±…ì •ë³´['url']

text = 'ì •ë³´ë¥¼ ëª¨ìœ¼ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤.'
my_bar = st.progress(0, text=text)

# ì˜µì…˜ ìƒì„±
options = webdriver.ChromeOptions()
# ì°½ ìˆ¨ê¸°ëŠ” ì˜µì…˜ ì¶”ê°€
options.add_argument("headless")

driver = webdriver.Chrome(options=options)
driver.get(target_url)
time.sleep(5)


try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[2]/div[3]/a')
    botton.click()
except :
    pass
ì±…ì†Œê°œ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[2]/p')

time.sleep(3)
try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[5]/div[3]/a')
    botton.click()
except :
    pass
ì±…ì†ìœ¼ë¡œ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[5]/p')


time.sleep(3)
try :
    botton = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[6]/div[3]/a')
    botton.click()
except :
    pass
ì„œí‰ = driver.find_element(By.XPATH, '//*[@id="tabContent"]/div[1]/div[6]/p')


book['ì±…ì†Œê°œ'] = ì±…ì†Œê°œ.text
book['ì±…ì†ìœ¼ë¡œ'] = ì±…ì†ìœ¼ë¡œ.text
book['ì„œí‰'] = ì„œí‰.text
driver.close()

time.sleep(1)
my_bar.progress(30, text=text)


#ì˜ì–´ ë¶ˆìš©ì–´ ì‚¬ì „
stops = set(stopwords.words('english'))

def hapus_url(text):
    mention_pattern = r'@[\w]+'
    cleaned_text = re.sub(mention_pattern, '', text)
    return re.sub(r'http\S+','', cleaned_text)

#íŠ¹ìˆ˜ë¬¸ì ì œê±°
#ì˜ì–´ ëŒ€ì†Œë¬¸ì, ìˆ«ì, ê³µë°±ë¬¸ì(ìŠ¤í˜ì´ìŠ¤, íƒ­, ì¤„ë°”ê¿ˆ ë“±) ì•„ë‹Œ ë¬¸ìë“¤ ì œê±°
def remove_special_characters(text, remove_digits=True):
    text=re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return text


#ë¶ˆìš©ì–´ ì œê±°
def delete_stops(text):
    text = text.lower().split()
    text = ' '.join([word for word in text if word not in stops])
    return text
   
    
#í’ˆì‚¬ tag ë§¤ì¹­ìš© í•¨ìˆ˜
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
    

#í’ˆì‚¬ íƒœê¹… + í‘œì œì–´ ì¶”ì¶œ
def tockenize(text):
    tokens=word_tokenize(text)
    pos_tokens=nltk.pos_tag(tokens)
    
    text_t=list()
    for _ in pos_tokens:
        text_t.append([_[0], get_wordnet_pos(_[1])])
    
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word[0], word[1]) for word in text_t])
    return text

def clean(text):
    text = remove_special_characters(text, remove_digits=True)
    text = delete_stops(text)
    text = tockenize(text)
    return text

my_bar.progress(50, text=text)

translator = Translator()
for col in ['ì±…ì†Œê°œ', 'ì±…ì†ìœ¼ë¡œ', 'ì„œí‰']:
    name = col+'_trans'
    if book[col].values == '':
        book[name] = ''
        continue
    book[name] = clean(translator.translate(hapus_url(book.loc[0, col])).text)

total_text = book.loc[0, 'ì±…ì†Œê°œ_trans'] + book.loc[0, 'ì±…ì†ìœ¼ë¡œ_trans'] + book.loc[0, 'ì„œí‰_trans']

df = pd.read_csv('tweet_data_agumentation.csv', index_col = 0)

tfidf_vect_emo = TfidfVectorizer()
tfidf_vect_emo.fit_transform(df["content"])

model = joblib.load('SVM.pkl')
total_text2 = tfidf_vect_emo.transform(pd.Series(total_text))
model.predict_proba(total_text2)
sentiment = pd.DataFrame(model.predict_proba(total_text2),index=['prob']).T
sentiment['ê°ì •'] = ['empty','sadness','enthusiasm','worry','love','fun','hate','happiness','boredom','relief','anger']
sentiment2 = sentiment.sort_values(by='prob',ascending=False)

my_bar.progress(60, text=text)

# audio featureë‘ text ê°ì •
audio_data = data.iloc[:,-12:-1]

sentiment_prob = sentiment['prob']
sentiment_prob.index = sentiment['ê°ì •']

audio_data.columns = ['empty', 'sadness', 'enthusiasm', 'worry', 'love', 'fun', 'hate',
       'happiness', 'boredom', 'relief', 'anger']

audio_data_1 = pd.concat([sentiment_prob,audio_data.T],axis=1).T

col = ['book']+list(data['name'])
cosine_sim_audio = cosine_similarity(audio_data_1)
cosine_sim_audio_df = pd.DataFrame(cosine_sim_audio, index = col, columns=col)

audio_sim = cosine_sim_audio_df['book']
my_bar.progress(70, text=text)

# ê°€ì‚¬ë‘ text
lyrics_data = data.iloc[:,5:-12]
lyrics_data_1 = pd.concat([sentiment_prob,lyrics_data.T],axis=1).T
cosine_sim_lyrics = cosine_similarity(lyrics_data_1)
cosine_sim_lyrics_df = pd.DataFrame(cosine_sim_lyrics, index =col, columns=col)
lyrics_sim = cosine_sim_lyrics_df['book']
my_bar.progress(80, text=text)

# í‚¤ì›Œë“œë‘ text
keyword_data = data['key_word']
book_song_cont1 = pd.DataFrame({"text": total_text}, index = range(1))
book_song_cont2 = pd.DataFrame({"text": keyword_data})
keyword_data_1 = pd.concat([book_song_cont1, book_song_cont2], axis=0).reset_index(drop=True)

tfidf_vect_cont = TfidfVectorizer()
tfidf_matrix_cont = tfidf_vect_cont.fit_transform(keyword_data_1['text'])
tfidf_array_cont = tfidf_matrix_cont.toarray()
tfidf_df_cont = pd.DataFrame(tfidf_array_cont, columns=tfidf_vect_cont.get_feature_names_out())

cosine_sim_keyword = cosine_similarity(tfidf_array_cont)
cosine_sim_keyword_df = pd.DataFrame(cosine_sim_keyword, index = col, columns=col)
keyword_sim = cosine_sim_keyword_df['book']
my_bar.progress(90, text=text)

# ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
total_sim  = 0.8*audio_sim + 0.1*lyrics_sim + 0.1*keyword_sim

recommend_song = total_sim.sort_values(ascending=False)[1:6].inde-x
total_sim_df = pd.DataFrame(total_sim[1:])
total_sim_df = total_sim_df.reset_index()
total_sim_df.columns = ['name','book']

top_five = total_sim_df.sort_values(by='book',ascending=False)[:5]
index = total_sim_df.sort_values(by='book',ascending=False)[:5].index.sort_values()
artist = data.iloc[index][['url','name','Artist']]
top_five_df = pd.merge(artist,top_five,on='name').sort_values(by='book',ascending=False).drop_duplicates()

my_bar.progress(100, text=text)
time.sleep(1)
my_bar.empty()

st.dataframe(top_five_df)